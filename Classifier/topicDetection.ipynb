{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pip\n",
    "pip.main(['install', '--user', 'nltk'])\n",
    "\n",
    "import warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\",category=FutureWarning)\n",
    "    import sklearn\n",
    "    from sklearn.feature_extraction import DictVectorizer\n",
    "    import sklearn.feature_extraction.text\n",
    "    from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "    from sklearn import metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import topbox\n",
    "import sklearn\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "import sklearn.feature_extraction.text\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn import metrics\n",
    "\n",
    "%matplotlib inline\n",
    "%run plot_learning_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    " \n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('universal_tagset')\n",
    "# The files are coded in ISO-8859-1\n",
    "\n",
    "df = pd.read_csv(\"tweetsCSV/Esp/Theme/themeTrain.csv\", encoding='utf-8', delimiter=\",\", header=0)\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical variables\n",
    "\n",
    "df.loc[df[\"Themes\"]==\"a\",\"Themes\"] = 0\n",
    "df.loc[df[\"Themes\"]==\"b\",\"Themes\"] = 1\n",
    "df.loc[df[\"Themes\"]==\"c\",\"Themes\"] = 2\n",
    "df.loc[df[\"Themes\"]==\"d\",\"Themes\"] = 3\n",
    "df.loc[df[\"Themes\"]==\"e\",\"Themes\"] = 4\n",
    "\n",
    "df['Themes'] = df['Themes'].astype(np.int64)\n",
    "\n",
    "df.dtypes\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define X and Y\n",
    "X = df['text'].values\n",
    "y = df['Themes'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lexical feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample of statistics using nltk\n",
    "# Another option is defining a function and pass it as a parameter to FunctionTransformer\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "class LexicalStats (BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract lexical features from each document\"\"\"\n",
    "    \n",
    "    def number_sentences(self, doc):\n",
    "        sentences = sent_tokenize(doc, language='spanish')\n",
    "        return len(sentences)\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, docs):\n",
    "        return [{'length': len(doc),\n",
    "                 'num_sentences': self.number_sentences(doc)}\n",
    "                for doc in docs]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import string\n",
    "\n",
    "def custom_tokenizer(words):\n",
    "    \"\"\"Preprocessing tokens as seen in the lexical notebook\"\"\"\n",
    "    \n",
    "\n",
    "    urls = re.compile(r'.http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    ht = re.compile(r'http.')\n",
    "    bar = re.compile(r'//*')\n",
    "    pr = [\"rt\",\"@\",\"http\",\"https\",\"'s\",'...', 'english', 'translation','):', '. .', '..']\n",
    "    #tknzr = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "    tknzr = TweetTokenizer(strip_handles=False, reduce_len=True)\n",
    "    #tokens = word_tokenize(words.lower())\n",
    "    tokens = tknzr.tokenize(words.lower())\n",
    "    porter = PorterStemmer()\n",
    "    lemmas = [porter.stem(t) for t in tokens]\n",
    "    # Clean stop-words\n",
    "    stoplist = stopwords.words('spanish')\n",
    "    lemmas_clean = [w for w in lemmas if w not in stoplist]\n",
    "    # Clean punctuation\n",
    "    punctuation = set(string.punctuation)\n",
    "    lemmas_punct = [w for w in lemmas_clean if w=='?'or w not in punctuation]\n",
    "    # Clean emojis,urls,bars,etc\n",
    "    lemmas_clean = [w for w in lemmas_punct if w!=\"insomnio\" if not w.startswith('@') if w not in pr \n",
    "            if not bar.search(w) if not ht.search(w)\n",
    "            if not w.isdigit()]\n",
    "    \n",
    "    return lemmas_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Syntatic features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from nltk import pos_tag\n",
    "from collections import Counter \n",
    "\n",
    "class PosStats(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Obtain number of tokens with POS categories\"\"\"\n",
    "\n",
    "    def stats(self, doc):\n",
    "        tokens = custom_tokenizer(doc)\n",
    "        tagged = pos_tag(doc, tagset='universal')\n",
    "        counts = Counter(tag for word,tag in tagged)\n",
    "        total = sum(counts.values())\n",
    "        #copy tags so that we return always the same number of features\n",
    "        pos_features = {'NOUN': 0, 'ADJ': 0, 'VERB': 0, 'ADV': 0, 'CONJ': 0, \n",
    "                        'ADP': 0, 'PRON':0, 'NUM': 0}\n",
    "        \n",
    "        pos_dic = dict((tag, float(count)/total) for tag,count in counts.items())\n",
    "        for k in pos_dic:\n",
    "            if k in pos_features:\n",
    "                pos_features[k] = pos_dic[k]\n",
    "        return pos_features\n",
    "    \n",
    "    def transform(self, docs, y=None):\n",
    "        return [self.stats(tweet) for tweet in docs]\n",
    "    \n",
    "    def fit(self, docs, y=None):\n",
    "        \"\"\"Returns `self` unless something different happens in train and test\"\"\"\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "\n",
    "ngrams_featurizer = Pipeline([\n",
    "  ('count_vectorizer',  CountVectorizer(ngram_range = (1, 3), encoding = 'utf-8', \n",
    "                                        tokenizer=custom_tokenizer)),\n",
    "  ('tfidf_transformer', TfidfTransformer())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopicTopWords(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def loadWords(self, file):\n",
    "        df=pd.read_csv(file, encoding='utf-8', delimiter=\",\", header=0)\n",
    "        df=df.set_index(\"words\")\n",
    "        dic=[]\n",
    "        dic=dict((word,prob.values[0]) for word,prob in df.iterrows())\n",
    "    \n",
    "        return dic\n",
    "    \n",
    "    def topWords(self, tweet, resultlist):\n",
    "        topWords_a=self.loadWords(\"tweetsCSV/Esp/Theme/topWords_a.csv\")\n",
    "        topWords_b=self.loadWords(\"tweetsCSV/Esp/Theme/topWords_b.csv\")\n",
    "        topWords_c=self.loadWords(\"tweetsCSV/Esp/Theme/topWords_c.csv\")\n",
    "        topWords_d=self.loadWords(\"tweetsCSV/Esp/Theme/topWords_d.csv\")\n",
    "        topWords_e=self.loadWords(\"tweetsCSV/Esp/Theme/topWords_e.csv\")\n",
    "        #allhtdict = dict((ht, 0) for ht in listallht)\n",
    "        #sent = tknzrwhu.tokenize(str(tweet))       \n",
    "        prob_a=0\n",
    "        prob_b=0\n",
    "        prob_c=0\n",
    "        prob_d=0\n",
    "        prob_e=0\n",
    "        for term in tweet:\n",
    "            if term in topWords_a.keys():\n",
    "                prob_a+=topWords_a.get(term)\n",
    "            if term in topWords_b.keys():\n",
    "                prob_b+=topWords_b.get(term)\n",
    "            if term in topWords_c.keys():\n",
    "                prob_c+=topWords_c.get(term)\n",
    "            if term in topWords_d.keys():\n",
    "                prob_d+=topWords_d.get(term)\n",
    "            if term in topWords_e.keys():\n",
    "                prob_e+=topWords_e.get(term)\n",
    "        theme_dict={\"a\":prob_a,\"b\":prob_b,\"c\":prob_c,\"d\":prob_d,\"e\":prob_e}\n",
    "        #print(tweet)\n",
    "        #print(theme_dict)\n",
    "        resultlist.append(theme_dict)\n",
    "        \n",
    "        return (resultlist)\n",
    "\n",
    "\n",
    "    def transform(self, data):\n",
    "        #print(\"Entra en hashtags\")\n",
    "        #dataproc = preProcess(data)\n",
    "        #lista = []\n",
    "        listaresultado = []\n",
    "        for tweet in data:\n",
    "            tweet_processed=custom_tokenizer(tweet)\n",
    "        #for tweet in dataproc:\n",
    "        #    lista = self.allHashtags(tweet, lista)\n",
    "        #for tweet in dataproc:\n",
    "        #    listaresultado = self.Hashtags(tweet, lista, listaresultado)\n",
    "            listaresultado=self.topWords(tweet_processed,listaresultado)\n",
    "        #print(listaresultado)\n",
    "        return listaresultado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Union Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.cross_validation import cross_val_score, KFold\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, accuracy_score, make_scorer\n",
    "\n",
    "def pipeline(clf):\n",
    "    return Pipeline([\n",
    "       ('features', FeatureUnion([\n",
    "                    ('lexical_stats', Pipeline([\n",
    "                                ('stats', LexicalStats()),\n",
    "                                ('vectors', DictVectorizer())\n",
    "                            ])),\n",
    "                    ('words', TfidfVectorizer(tokenizer=custom_tokenizer)),\n",
    "                    ('ngrams', ngrams_featurizer),\n",
    "                    # Topics of the Docs\n",
    "                    ('label-lda', Pipeline([\n",
    "                                ('topWords', TopicTopWords()),\n",
    "                                ('vect', DictVectorizer())\n",
    "                            ])),\n",
    "                ])),\n",
    "        # Machine Learning\n",
    "        ('clf', clf)  # classifier\n",
    "        #('clf', SVC(gamma= 3, kernel='linear', probability=True))\n",
    "\n",
    "    ])\n",
    "\n",
    "def classification_report_with_accuracy_score(y_true, y_pred):\n",
    "\n",
    "    print (classification_report(y_true, y_pred)) # print classification report\n",
    "    return accuracy_score(y_true, y_pred) # return accuracy score\n",
    "\n",
    "cv = KFold(X.shape[0], n_folds=5, shuffle=False, random_state=33)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, Optimize and Evaluate models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "#Optimize multinomialNB\n",
    "pipelineNB = pipeline(MultinomialNB(alpha=.001))\n",
    "\n",
    "parametersNB = {'clf__alpha': [.0001,.001,.01,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]}\n",
    "\n",
    "gs_NB = GridSearchCV(pipelineNB, parametersNB, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_NB = gs_NB.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best Score with MultinomialNB: %s\" % gs_NB.best_score_)\n",
    "for param_name in sorted(parametersNB.keys()):\n",
    "    print(\"%s: %r\" % (param_name, gs_NB.best_params_[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate with K-Fold\n",
    "model_NB = pipeline(MultinomialNB(alpha=gs_NB.best_params_['clf__alpha'] ))\n",
    "scores = cross_val_score(model_NB, X, y, cv=cv)\n",
    "print(\"Scores in every iteration\", scores)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curve(model_NB, \"Learning curve with K-Fold\", X, y, cv=cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nested CV with parameter optimization\n",
    "nested_score = cross_val_score(model_NB, X, y, cv=cv, \\\n",
    "               scoring=make_scorer(classification_report_with_accuracy_score))\n",
    "print (nested_score) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#Optimize SVC\n",
    "pipelineSVC = pipeline(SVC(C=1,gamma= 3, kernel='linear', probability=True))\n",
    "\n",
    "parametersSVC = {'clf__C':range(1,15),'clf__gamma': np.logspace(-6, -1, 10), 'clf__kernel': ('linear','rbf'),\n",
    "                 'clf__probability':(True,False),}\n",
    "\n",
    "gs_SVC = GridSearchCV(pipelineSVC, parametersSVC, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_SVC= gs_SVC.fit(X,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best Score with SVC: %s\" % gs_SVC.best_score_)\n",
    "for param_name in sorted(parametersSVC.keys()):\n",
    "    print(\"%s: %r\" % (param_name, gs_SVC.best_params_[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate with K-Fold\n",
    "C_SVC=gs_SVC.best_params_['clf__C']\n",
    "gamma_SVC = gs_SVC.best_params_['clf__gamma']\n",
    "kernel_SVC = gs_SVC.best_params_['clf__kernel']\n",
    "probability_SVC = gs_SVC.best_params_['clf__probability']\n",
    "model_SVC = pipeline(SVC(C=C_SVC,gamma=gamma_SVC, kernel=kernel_SVC, probability=probability_SVC))\n",
    "scores = cross_val_score(model_SVC, X, y, cv=cv)\n",
    "print(\"Scores in every iteration\", scores)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curve(model_SVC, \"Learning curve with K-Fold\", X, y, cv=cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nested CV with parameter optimization\n",
    "nested_score = cross_val_score(model_SVC, X, y, cv=cv, \\\n",
    "               scoring=make_scorer(classification_report_with_accuracy_score))\n",
    "print (nested_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNeighbourClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#Optimize KNeighborsClassifier\n",
    "pipelineKN = pipeline(KNeighborsClassifier(n_neighbors=3)) \n",
    "parametersKN = {'clf__n_neighbors': range(1,25), 'clf__p':(1,2)}\n",
    "\n",
    "gs_KN = GridSearchCV(pipelineKN, parametersKN, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_KN= gs_KN.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best Score with KN: %s\" % gs_KN.best_score_)\n",
    "for param_name in sorted(parametersKN.keys()):\n",
    "    print(\"%s: %r\" % (param_name, gs_KN.best_params_[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate with K-Fold\n",
    "model_KN = pipeline(KNeighborsClassifier(gs_KN.best_params_['clf__n_neighbors'],p=gs_KN.best_params_['clf__p']))\n",
    "scores = cross_val_score(model_KN, X, y, cv=cv)\n",
    "print(\"Scores in every iteration\", scores)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nested CV with parameter optimization\n",
    "nested_score = cross_val_score(model_KN, X, y, cv=cv, \\\n",
    "               scoring=make_scorer(classification_report_with_accuracy_score))\n",
    "print (nested_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "#Optimize LogisticRegresion\n",
    "pipelineLR =  pipeline(LogisticRegression(penalty='l2',tol=0.0001,C=1.0,n_jobs=-1)) \n",
    "parametersLR = {'clf__penalty': ['l1','l2'], 'clf__tol': [0.0001,0.001,0.01,0.1], 'clf__C': range(1,15)}\n",
    "\n",
    "gs_LR = GridSearchCV(pipelineLR, parametersLR, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_LR= gs_LR.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best Score with LogisticRegression: %s\" % gs_LR.best_score_)\n",
    "for param_name in sorted(parametersLR.keys()):\n",
    "    print(\"%s: %r\" % (param_name, gs_LR.best_params_[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate with K-Fold\n",
    "#penalty_LR=gs_LR.best_params_['clf__penalty']\n",
    "#tol_LR = gs_LR.best_params_['clf__tol']\n",
    "#C_LR = gs_LR.best_params_['clf__C']\n",
    "#model_LR = pipeline(LogisticRegression(penalty=penalty_LR,tol=tol_LR,C=C_LR,n_jobs=-1))\n",
    "model_LR = pipeline(LogisticRegression(penalty='l2',tol=0.01,C=14,n_jobs=-1))\n",
    "scores = cross_val_score(model_LR, X,y, cv=cv)\n",
    "print(\"Scores in every iteration\", scores)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curve(model_LR, \"Learning curve with K-Fold\", X, y, cv=cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nested CV with parameter optimization\n",
    "nested_score = cross_val_score(model_LR, X, y, cv=cv, \\\n",
    "               scoring=make_scorer(classification_report_with_accuracy_score))\n",
    "print (nested_score) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train classifier\n",
    "model_LR.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "extracted_features = model_LR.named_steps['features'].transform(X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse\n",
    "\n",
    "scipy.sparse.save_npz('./extracted_features_themes.npz',extracted_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=model_LR.predict([\"Mi tratamiento contra el insomnio es no dormir siesta\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#Optimize RandomForests\n",
    "pipelineRF = pipeline(RandomForestClassifier(n_estimators=10,n_jobs=-1))\n",
    "\n",
    "parametersRF = {'clf__n_estimators': range (1,25)}\n",
    "\n",
    "gs_RF = GridSearchCV(pipelineRF, parametersRF, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_RF= gs_RF.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best Score with RandomForests: %s\" % gs_RF.best_score_)\n",
    "for param_name in sorted(parametersRF.keys()):\n",
    "    print(\"%s: %r\" % (param_name, gs_RF.best_params_[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate with K-Fold\n",
    "estimators_RF=gs_RF.best_params_['clf__n_estimators']\n",
    "\n",
    "model_RF = pipeline(RandomForestClassifier(n_estimators=estimators_RF,n_jobs=-1))\n",
    "scores = cross_val_score(model_RF, X,y, cv=cv)\n",
    "print(\"Scores in every iteration\", scores)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nested CV with parameter optimization\n",
    "nested_score = cross_val_score(model_RF, X, y, cv=cv, \\\n",
    "               scoring=make_scorer(classification_report_with_accuracy_score))\n",
    "print (nested_score) \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
